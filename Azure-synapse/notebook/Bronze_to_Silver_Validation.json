{
	"name": "Bronze_to_Silver_Validation",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "projsparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "8e0fd1d3-81aa-4794-9d7f-1fd135bfb14e"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/49a5e15e-626c-4e4c-8752-d06f2a37c581/resourceGroups/rg_azure_practice/providers/Microsoft.Synapse/workspaces/ws-azuresynapse/bigDataPools/projsparkpool",
				"name": "projsparkpool",
				"type": "Spark",
				"endpoint": "https://ws-azuresynapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/projsparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.5",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Step 1: Read Bronze Parquet"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"bronze_path = \"abfss://con-healthcare-proj@saazurepractice12.dfs.core.windows.net/bronze/\"\n",
					"\n",
					"patients_df = spark.read.parquet(f\"{bronze_path}patients/\")\n",
					"admissions_df = spark.read.parquet(f\"{bronze_path}admissions/\")\n",
					"diagnosis_df = spark.read.parquet(f\"{bronze_path}diagnoses_icd/\")\n",
					"insurance_df = spark.read.parquet(f\"{bronze_path}insurance/\")\n",
					"doctors_df = spark.read.parquet(f\"{bronze_path}Doctors/\")\n",
					"departments_df = spark.read.parquet(f\"{bronze_path}Departments/\")\n",
					"d_icd_diagnoses = spark.read.parquet(f\"{bronze_path}d_icd_diagnoses/\")\n",
					""
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Step 2: Schema Validation"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"##### 2.1 â€” Define Expected Schema"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.types import *\n",
					"\n",
					"all_schemas = {\n",
					"#--patients\n",
					"    \"patients\" : StructType([\n",
					"        StructField(\"subject_id\", StringType(), False),  \n",
					"        StructField(\"gender\", StringType(), True),\n",
					"        StructField(\"anchor_age\", IntegerType(), True),\n",
					"        StructField(\"anchor_year\", IntegerType(), True)\n",
					"    ]),\n",
					"\n",
					"#--admissions\n",
					"    \"admissions\" : StructType([\n",
					"        StructField(\"hadm_id\", StringType(), False),\n",
					"        StructField(\"subject_id\", StringType(), False),\n",
					"        StructField(\"admittime\", TimestampType(), True),\n",
					"        StructField(\"dischtime\", TimestampType(), True)\n",
					"    ]),\n",
					"\n",
					"#--diagnosis_icd\n",
					"    \"diagnoses_icd\" : StructType([\n",
					"        StructField(\"hadm_id\", StringType(), False),\n",
					"        StructField(\"icd_code\", StringType(), True)\n",
					"    ]),\n",
					"\n",
					"#--d_icd_diagnoses\n",
					"    \"d_icd_diagnoses\" : StructType([\n",
					"        StructField(\"icd_code\", StringType(), True),\n",
					"        StructField(\"long_title\", StringType(), True)\n",
					"    ]),\n",
					"\n",
					"#--insurance\n",
					"    \"insurance\" : StructType([\n",
					"        StructField(\"age\", IntegerType(), True),\n",
					"        StructField(\"sex\", StringType(),True),\n",
					"        StructField(\"bmi\", DoubleType(),True),\n",
					"        StructField(\"children\", IntegerType(), True),\n",
					"        StructField(\"smoker\", StringType(),True),\n",
					"        StructField(\"region\", StringType(), True),\n",
					"        StructField(\"charges\", DoubleType(), True)\n",
					"    ]),\n",
					"\n",
					"#--doctors\n",
					"    \"doctors\" : StructType([\n",
					"        StructField(\"doctor_id\", StringType(),False),\n",
					"        StructField(\"doctor_name\", StringType(),True),\n",
					"        StructField(\"specialization\", StringType(), True),\n",
					"        StructField(\"experience_years\", IntegerType(), True),\n",
					"        StructField(\"qualification\", StringType(),True),\n",
					"        StructField(\"joining_date\", DateType(), True),\n",
					"        StructField(\"city\", StringType(), True),\n",
					"        StructField(\"active_flag\", StringType(), True)\n",
					"    ]),\n",
					"\n",
					"#--departments\n",
					"    \"departments\" : StructType([\n",
					"        StructField(\"department_id\", StringType(), True),\n",
					"        StructField(\"department_name\", StringType(), True),\n",
					"        StructField(\"department_head\", StringType(), True),\n",
					"        StructField(\"floor_number\", IntegerType(), True),\n",
					"        StructField(\"building\", StringType(), True),\n",
					"        StructField(\"contact_number\", StringType(), True),\n",
					"        StructField(\"opening_time\", StringType(), True),\n",
					"        StructField(\"closing_time\", StringType(), True),\n",
					"        StructField(\"active_flag\", StringType(), True), \n",
					"    ])\n",
					"}\n",
					"\n",
					"\n",
					"#patients_df = spark.read.schema(patients_schema).parquet(bronze_path + \"patients/\") --> Manually defining schema is correct, But NOT while reading Parquet."
				],
				"execution_count": 17
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### 2.2 - Schema Validation"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import col\n",
					"\n",
					"expected_schema = all_schemas[\"patients\"]\n",
					"\n",
					"patients_df = patients_df.withColumn(\"anchor_age\",col(\"anchor_age\").cast(\"int\"))\n",
					"#patients_df = patients_df.drop(\"gender\") \n",
					"#patients_df.show()\n",
					"\n",
					"for field in expected_schema:\n",
					"    if field.name not in patients_df.columns:\n",
					"        raise Exception(f\"Missing column {field.name}\")\n",
					""
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"source": [
					"expected_schema = all_schemas[\"admissions\"]\n",
					"admissions_df = admissions_df.withColumn(\"admittime\",col(\"admittime\").cast(\"timestamp\"))\n",
					"#admissions_df.show(truncate=False)\n",
					"\n",
					"for field in expected_schema:\n",
					"    if field.name not in admissions_df.columns:\n",
					"        raise Exception(f\"Missing column {field.name}\")"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"source": [
					"expected_schema = all_schemas[\"diagnoses_icd\"]\n",
					"\n",
					"for field in expected_schema:\n",
					"    if field.name not in diagnosis_df.columns:\n",
					"        raise Exception(f\"Missing column {field.name}\")"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"source": [
					"expected_schema = all_schemas[\"d_icd_diagnoses\"]\n",
					"\n",
					"d_icd_diagnoses = d_icd_diagnoses.filter(col(\"icd_code\").isNotNull()).dropDuplicates([\"icd_code\"])\n",
					"\n",
					"for field in expected_schema:\n",
					"    if field.name not in d_icd_diagnoses.columns:\n",
					"        raise Exception(f\"Missing column {field.name}\")\n",
					""
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"source": [
					"expected_schema = all_schemas[\"insurance\"]\n",
					"\n",
					"insurance_df = insurance_df.filter(col(\"charges\") > 0)\n",
					"\n",
					"for field in expected_schema:\n",
					"    if field.name not in insurance_df.columns:\n",
					"        raise Exception(f\"Missing column {field.name}\")\n",
					"\n",
					""
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"source": [
					"expected_schema = all_schemas[\"doctors\"]\n",
					"doctors_df.show()\n",
					"doctors_df = doctors_df.filter((col(\"doctor_id\").isNotNull()) & (col(\"active_flag\").isin(\"Y\", \"N\"))).dropDuplicates([\"doctor_id\"])\n",
					"\n",
					"if doctors_df.filter(~col(\"active_flag\").isin(\"Y\", \"N\")).count() > 0:\n",
					"    raise Exception(\"Invalid value in active_flag column. It must be Y or N\")\n",
					"\n",
					"for field in expected_schema:\n",
					"    if field.name not in doctors_df.columns:\n",
					"        raise Exception(f\"Missing column {field.name}\")\n",
					"\n",
					""
				],
				"execution_count": 18
			}
		]
	}
}